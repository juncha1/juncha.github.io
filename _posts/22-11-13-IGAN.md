---
layout: post
title:  "Improved Techniques for Training GANs"
date: "2022-11-13"  
author: ""
categories: "GAN"
use_math: "true"
---

# Improved Techniques for Training GANs

## 1. Introduction

Generative adversarial networks(GANs)은 game 이론에 근거한 생성 모델이다. 

(게임이론의 핵심이라 할 수 있는 내쉬 균형은 각자가 상대방 대응에 따라, 
최선의 선택을 하고 자신의 선택을 바꾸지 않는 균형 상태를 말한다.

![image](https://user-images.githubusercontent.com/117826908/201459109-8891980c-b620-4f0d-b34d-aee5dd612fe4.png)

하지만, GANs 은 game 이론의 내쉬 균형을 찾기 보다는 비용함수의 low value를 착기 위해 gradient descent 방식을 사용한다.

즉, 이는 non-convex 비용함수와 고차원의 파라미터 공간에서 수렴을 보장하지 못한다.

본 논문에서는 GAN이 수렴이기 위한 5가지의 기술을 소개한다.


## 2. Toward Convergent GAN Training

5가지 기술은 다음과 같다.
 - Feature Matching
 - Minibatch discrimination
 - Historical averaging
 - One-sided label smoothing
 - Virtual batch normalization

## 3. Assessment of image quality

3-1. 먼저, Feature Matching이다.

- 기존 GAN의 generator의 목적함수를 아래의 것으로 사용한다. 

![image](https://user-images.githubusercontent.com/117826908/201459436-de9de73c-b96b-48db-81d2-45151214a01a.png)

이는 Generator에 새로운 목적함수 지정하여 오버트레이닝을 방지하고, GAN의 불안정성(insatbility)을 해결한다.

Generator에서 생성한 분포가 실제 데이터의 분포를 matching 시키기 위해 Discriminator 중간층의 activation 함수를 이용한다. 

단순하게 진짜/가짜를 나누는 방식이 아닌, 진짜와 같은 feature를 가지고 있느냐? 라는 방식으로 훈련을 진행하는 것이다. 

이를 위해 새로운 손실함수를 위화 같은 방식으로 정의하고 사용한다.

여기서 f(x)는 Discriminator의 중간 층 activation 함수다. 

식을 이해해보면 Discriminator 중간층의 output이 생성에 필요한 하나의 특징(feature)이며, 

이것이 random sampling된 z에 대해 분포가 비슷한지(matching) 살펴보는 것이다.

G가 목표하는 통계치에 도달하는지는 확신할 수 없지만, 경험적으로 불안정한 GAN에 대해 효과적이라고 이야기 하고 있다.

3-2. 다음으로 Minibatch discriminator다.

![image](https://user-images.githubusercontent.com/117826908/201459874-3aad27fe-1b19-407c-b724-7faa4e6dd3b4.png)




## 4. Semi-supervised learning

## 6. Experiments

## 7. Conclusion

1. Introduction
2. Related Work
3. Toward Convergent GAN Training
4. Assessment of image quality
5. Semi-supervised learning
6 Experiments 
7 Conclusion 

testt
